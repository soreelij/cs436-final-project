{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d183bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful link:\n",
    "# https://towardsdatascience.com/generating-scientific-papers-titles-using-machine-learning-98c8c9bc637e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f3a93b",
   "metadata": {},
   "source": [
    "# Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945d5572",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 16:29:53.799654: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "/Users/nataliehahle/opt/anaconda3/lib/libenchant-2.so does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnowball\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mstem\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Checking if a word is in the English Dictionary\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01menchant\u001b[39;00m\n\u001b[1;32m     17\u001b[0m d \u001b[38;5;241m=\u001b[39m enchant\u001b[38;5;241m.\u001b[39mDict(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_US\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Remove warnings\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/enchant/__init__.py:81\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menchant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _enchant \u001b[38;5;28;01mas\u001b[39;00m _e\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYENCHANT_IGNORE_MISSING_LIB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/enchant/_enchant.py:147\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# Last chance\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m from_system()\n\u001b[0;32m--> 147\u001b[0m enchant_lib_path \u001b[38;5;241m=\u001b[39m \u001b[43mfind_c_enchant_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enchant_lib_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m     msg \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124;03m\"\"\"\\\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m        The 'enchant' C library was not found and maybe needs to be installed.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/enchant/_enchant.py:137\u001b[0m, in \u001b[0;36mfind_c_enchant_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m library_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYENCHANT_LIBRARY_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m library_path:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_env_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlibrary_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m from_package \u001b[38;5;241m=\u001b[39m from_package_resources()\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_package:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/enchant/_enchant.py:79\u001b[0m, in \u001b[0;36mfrom_env_var\u001b[0;34m(library_path)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_env_var\u001b[39m(library_path):\n\u001b[1;32m     78\u001b[0m     find_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing PYENCHANT_LIBRARY_PATH env var\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(library_path), library_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m library_path\n",
      "\u001b[0;31mAssertionError\u001b[0m: /Users/nataliehahle/opt/anaconda3/lib/libenchant-2.so does not exist"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GRU\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Tagging expressions\n",
    "import nltk\n",
    "import re \n",
    "from nltk import pos_tag\n",
    "import nltk.stem.snowball as stem\n",
    "\n",
    "# Checking if a word is in the English Dictionary\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38982c8",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d7e5d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "album_name = pd.read_csv('album_text.csv')\n",
    "album_name = album_name.set_index('Unnamed: 0')\n",
    "album_name.dropna(inplace=True)\n",
    "print(len(album_name))\n",
    "album_name.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3beab698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word\n",
       "0  the\n",
       "1   of\n",
       "2  and\n",
       "3   to\n",
       "4    a"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gets top 10,000 most common words (filters out subtle words that are not in english)\n",
    "_20k = pd.read_csv('10k.txt',delimiter='\\t', names=['word','la'])\n",
    "_20k = _20k.drop(columns=['la'])\n",
    "relevant = _20k['word'].tolist()\n",
    "_20k.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1185ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20z = pd.read_csv('top_tags.csv')\n",
    "top_20z.head(5)\n",
    "top_20 = top_20z['Top Tags']\n",
    "top_20 = top_20.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb99f3",
   "metadata": {},
   "source": [
    "# Remove all non-English Text and remove weird characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c4d73fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "# Get english text only\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        for w in s:\n",
    "            chars = set('({=~<=@#=%&+,-/:>[.*_|')\n",
    "            if any((c in chars) for c in w):\n",
    "                return False\n",
    "#         return True\n",
    "        # bypasses other languages\n",
    "        for w in nltk.wordpunct_tokenize(s):\n",
    "            if w.lower() in words or not w.isalpha():\n",
    "                i = 0\n",
    "            else:\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263f105f",
   "metadata": {},
   "source": [
    "# Remove all non-English Text and remove weird characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52686060",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113999\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album_name</th>\n",
       "      <th>english?</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comedy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ghost (Acoustic)</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crazy Rich Asians (Original Motion Picture Sou...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hold On</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   album_name  english?\n",
       "Unnamed: 0                                                             \n",
       "0                                                      Comedy      True\n",
       "1                                            Ghost (Acoustic)     False\n",
       "2                                              To Begin Again      True\n",
       "3           Crazy Rich Asians (Original Motion Picture Sou...     False\n",
       "4                                                     Hold On      True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filters to English text only\n",
    "album_name['english?'] = [isEnglish(name) for name in album_name['album_name']]\n",
    "print(len(album_name))\n",
    "album_name.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03f604f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29216\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album_name</th>\n",
       "      <th>english?</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comedy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hold On</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Days I Will Remember</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Is There Anybody Out There?</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             album_name  english?\n",
       "Unnamed: 0                                       \n",
       "0                                Comedy      True\n",
       "2                        To Begin Again      True\n",
       "4                               Hold On      True\n",
       "5                  Days I Will Remember      True\n",
       "6           Is There Anybody Out There?      True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "album_name = album_name[album_name['english?'] == True]\n",
    "print(len(album_name))\n",
    "# makes it a list\n",
    "name_list = album_name['album_name'].tolist() \n",
    "album_name.head(5)\n",
    "# name_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84566b0",
   "metadata": {},
   "source": [
    "# All album names to lower and in a list (also removed duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b02a86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tech power',\n",
       " 'pure frosting',\n",
       " 'isolate and medicate',\n",
       " 'more real folk blues',\n",
       " 'sub stele',\n",
       " 'complex',\n",
       " \"it's been too long\",\n",
       " 'level days',\n",
       " 'disappearing now',\n",
       " 'recon']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_lower = []\n",
    "# case lowering of the entire list\n",
    "for name in name_list:\n",
    "    low = name.lower()\n",
    "    name_lower.append(low)\n",
    "# removes duplicates\n",
    "name_lower = list(set(name_lower))\n",
    "print(len(name_lower))\n",
    "name_lower[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5278e870",
   "metadata": {},
   "source": [
    "# Gets unique characters and adds to a vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f494f9b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 unique characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([' ', '!', '\"', '$', \"'\", '0', '1', '2', '3', '4', '5', '6', '7',\n",
       "       '8', '9', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j',\n",
       "       'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w',\n",
       "       'x', 'y', 'z'], dtype='<U1')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = name_lower\n",
    "text = ''\n",
    "for t in terms:\n",
    "    text=text+' ' +t\n",
    "text = text[1::]\n",
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "idx2char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81edbdc5",
   "metadata": {},
   "source": [
    "# Gets training/test and builds a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3323dfdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 22:29:17.183443: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(32, 100), dtype=tf.int64, name=None), TensorSpec(shape=(32, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "# Batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff69e68",
   "metadata": {},
   "source": [
    "# Builds a model with the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f0f8d95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (32, None, 100)           4200      \n",
      "                                                                 \n",
      " gru (GRU)                   (32, None, 150)           113400    \n",
      "                                                                 \n",
      " dense (Dense)               (32, None, 42)            6342      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 123,942\n",
      "Trainable params: 123,942\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 100\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 150\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,embedding_dim, batch_input_shape=[batch_size, None]))\n",
    "    model.add(GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\n",
    "    model.add(Dense(vocab_size))\n",
    "    \n",
    "  \n",
    "    return model\n",
    "model = build_model(42,embedding_dim, rnn_units, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d15ee19",
   "metadata": {},
   "source": [
    "# Adds the loss function and compiles with adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0e68e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss,run_eagerly=True)\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4464a",
   "metadata": {},
   "source": [
    "# Fits the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c78b27ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/48\n",
      "51/51 [==============================] - 18s 356ms/step - loss: 3.1397\n",
      "Epoch 2/48\n",
      "51/51 [==============================] - 18s 349ms/step - loss: 2.7128\n",
      "Epoch 3/48\n",
      "51/51 [==============================] - 18s 353ms/step - loss: 2.4567\n",
      "Epoch 4/48\n",
      "51/51 [==============================] - 20s 378ms/step - loss: 2.3603\n",
      "Epoch 5/48\n",
      "51/51 [==============================] - 20s 398ms/step - loss: 2.2890\n",
      "Epoch 6/48\n",
      "51/51 [==============================] - 19s 362ms/step - loss: 2.2263\n",
      "Epoch 7/48\n",
      "51/51 [==============================] - 20s 392ms/step - loss: 2.1708\n",
      "Epoch 8/48\n",
      "51/51 [==============================] - 19s 373ms/step - loss: 2.1226\n",
      "Epoch 9/48\n",
      "51/51 [==============================] - 20s 388ms/step - loss: 2.0790\n",
      "Epoch 10/48\n",
      "51/51 [==============================] - 19s 366ms/step - loss: 2.0399\n",
      "Epoch 11/48\n",
      "51/51 [==============================] - 19s 362ms/step - loss: 2.0039\n",
      "Epoch 12/48\n",
      "51/51 [==============================] - 18s 355ms/step - loss: 1.9720\n",
      "Epoch 13/48\n",
      "51/51 [==============================] - 18s 345ms/step - loss: 1.9415\n",
      "Epoch 14/48\n",
      "51/51 [==============================] - 18s 354ms/step - loss: 1.9114\n",
      "Epoch 15/48\n",
      "51/51 [==============================] - 21s 402ms/step - loss: 1.8834\n",
      "Epoch 16/48\n",
      "51/51 [==============================] - 20s 396ms/step - loss: 1.8596\n",
      "Epoch 17/48\n",
      "51/51 [==============================] - 19s 370ms/step - loss: 1.8379\n",
      "Epoch 18/48\n",
      "51/51 [==============================] - 18s 359ms/step - loss: 1.8157\n",
      "Epoch 19/48\n",
      "51/51 [==============================] - 18s 358ms/step - loss: 1.7963\n",
      "Epoch 20/48\n",
      "51/51 [==============================] - 18s 354ms/step - loss: 1.7777\n",
      "Epoch 21/48\n",
      "51/51 [==============================] - 18s 353ms/step - loss: 1.7592\n",
      "Epoch 22/48\n",
      "51/51 [==============================] - 19s 371ms/step - loss: 1.7435\n",
      "Epoch 23/48\n",
      "51/51 [==============================] - 21s 397ms/step - loss: 1.7278\n",
      "Epoch 24/48\n",
      "51/51 [==============================] - 19s 372ms/step - loss: 1.7135\n",
      "Epoch 25/48\n",
      "51/51 [==============================] - 20s 391ms/step - loss: 1.7003\n",
      "Epoch 26/48\n",
      "51/51 [==============================] - 21s 401ms/step - loss: 1.6866\n",
      "Epoch 27/48\n",
      "51/51 [==============================] - 21s 402ms/step - loss: 1.6747\n",
      "Epoch 28/48\n",
      "51/51 [==============================] - 21s 413ms/step - loss: 1.6635\n",
      "Epoch 29/48\n",
      "51/51 [==============================] - 21s 410ms/step - loss: 1.6524\n",
      "Epoch 30/48\n",
      "51/51 [==============================] - 21s 409ms/step - loss: 1.6424\n",
      "Epoch 31/48\n",
      "51/51 [==============================] - 21s 413ms/step - loss: 1.6328\n",
      "Epoch 32/48\n",
      "51/51 [==============================] - 21s 404ms/step - loss: 1.6231\n",
      "Epoch 33/48\n",
      "51/51 [==============================] - 20s 388ms/step - loss: 1.6147\n",
      "Epoch 34/48\n",
      "51/51 [==============================] - 21s 412ms/step - loss: 1.6048\n",
      "Epoch 35/48\n",
      "51/51 [==============================] - 22s 423ms/step - loss: 1.5961\n",
      "Epoch 36/48\n",
      "51/51 [==============================] - 22s 419ms/step - loss: 1.5891\n",
      "Epoch 37/48\n",
      "51/51 [==============================] - 21s 407ms/step - loss: 1.5803\n",
      "Epoch 38/48\n",
      "51/51 [==============================] - 20s 387ms/step - loss: 1.5743\n",
      "Epoch 39/48\n",
      "51/51 [==============================] - 20s 387ms/step - loss: 1.5672\n",
      "Epoch 40/48\n",
      "51/51 [==============================] - 20s 388ms/step - loss: 1.5608\n",
      "Epoch 41/48\n",
      "51/51 [==============================] - 20s 391ms/step - loss: 1.5528\n",
      "Epoch 42/48\n",
      "51/51 [==============================] - 21s 408ms/step - loss: 1.5454\n",
      "Epoch 43/48\n",
      "51/51 [==============================] - 21s 410ms/step - loss: 1.5401\n",
      "Epoch 44/48\n",
      "51/51 [==============================] - 21s 413ms/step - loss: 1.5327\n",
      "Epoch 45/48\n",
      "51/51 [==============================] - 21s 410ms/step - loss: 1.5260\n",
      "Epoch 46/48\n",
      "51/51 [==============================] - 21s 408ms/step - loss: 1.5203\n",
      "Epoch 47/48\n",
      "51/51 [==============================] - 21s 414ms/step - loss: 1.5145\n",
      "Epoch 48/48\n",
      "51/51 [==============================] - 21s 404ms/step - loss: 1.5105\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=48, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca4b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07a7ad",
   "metadata": {},
   "source": [
    "# Generates Text for a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba6a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "def generate_text(model, start_string,t):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 30\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temp = predictable text.\n",
    "    # Higher temp = more surprising text.\n",
    "    temperature = t\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c1f03b",
   "metadata": {},
   "source": [
    "# Testing model on sample phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9673379",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string='lolipops ',t=0.3))\n",
    "print(generate_text(model, start_string=u'days ',t=0.3))\n",
    "print(generate_text(model, start_string=u\"forever the \",t=0.3))\n",
    "print(generate_text(model, start_string=u\"no \",t=0.3))\n",
    "print(generate_text(model, start_string=u\"change \",t=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e5a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_one_word = []\n",
    "count = 0 \n",
    "while count < 50:\n",
    "    sss = generate_text(model, start_string='red',t=0.3)\n",
    "    gen_one_word.append(sss)\n",
    "    count = count + 1\n",
    "gen_names = pd.DataFrame()\n",
    "gen_names['temp: 0'] = gen_one_word\n",
    "# gen_names.head(5)\n",
    "\n",
    "# Parses through each generated temperature to get best generated output\n",
    "w = 0\n",
    "final_names = []\n",
    "while w < 1:\n",
    "    # =====================================================================\n",
    "    # Part I: Organizing/Finding Possible Tag Sequences\n",
    "    # =====================================================================\n",
    "    \n",
    "    # Gets the data from the right column\n",
    "    str_num = str(w)\n",
    "    column_name = 'temp: ' + str_num\n",
    "    temp = pd.DataFrame()\n",
    "    temp['name'] = gen_names[column_name]\n",
    "\n",
    "    # Parses the generated text into a list of tokens\n",
    "    lower = list(map(lambda x: x.lower().split(), temp['name']))\n",
    "    temp['Token'] = lower\n",
    "\n",
    "    # Gets the POS tagging of each word within the generated text\n",
    "    tag_sentence = list()\n",
    "    for array in lower:\n",
    "        tag = pos_tag(array)\n",
    "        tags_only = [x[1] for x in tag]\n",
    "        tag_sentence.append(tags_only) \n",
    "\n",
    "    # Turns the tag array into a tag string\n",
    "    tag_string = []\n",
    "    for listz in tag_sentence:\n",
    "        tag = \" \".join(listz)\n",
    "        tag_string.append(tag) \n",
    "    temp['Tag'] = tag_string\n",
    "\n",
    "    # Checks if a tag sequence of most popular titles matches a tag sequence of the generated title\n",
    "    greater_list = []\n",
    "    for gen_tag in temp['Tag']:\n",
    "        tags_found = []\n",
    "        for tag in top_20:\n",
    "            # Gets length of tag\n",
    "            tag_len = len(tag)\n",
    "            shortenz = gen_tag[0:tag_len]\n",
    "            # If the tag sequences match, add that tag to a list\n",
    "            if shortenz == tag:\n",
    "                tags_found.append(shortenz)\n",
    "        # Appends each found sub tag sequence to the title for tracking\n",
    "        greater_list.append(tags_found)\n",
    "    temp['Found'] = greater_list\n",
    "    # Makes a new DataFrame with the titles and their discovered tag sequence\n",
    "    newer = temp[['name','Found']].copy()\n",
    "\n",
    "    # Removes entries that had no found tag sequence\n",
    "    newer['Found'] = newer['Found'].apply(\n",
    "        lambda x: 'NaN' if len(x)==0 else x)\n",
    "    newer = newer[newer['Found'] != 'NaN']\n",
    "    \n",
    "    # =====================================================================\n",
    "    # Part II: Filtering/Refining the Generated Text\n",
    "    # =====================================================================\n",
    "    \n",
    "    # Filters out words that don't make sense/aren't in the English Dictionary\n",
    "    truth_list = []\n",
    "    for name in newer['name']:\n",
    "        # Each name starts out as True\n",
    "        nope = 1\n",
    "        name_list = name.split()\n",
    "        for word in name_list: \n",
    "            truth = d.check(word)\n",
    "            if word == 'i':\n",
    "                 nope = 0\n",
    "            # If word is not in the English Dictionary then flags\n",
    "            if truth is False:\n",
    "                nope = 0\n",
    "        truth_list.append(nope)\n",
    "    # Appends all flags to DataFrame and removes gibberish\n",
    "    newer['real'] = truth_list\n",
    "    filtered = newer[newer['real'] == 1]\n",
    "\n",
    "    # Grabs the length of the tag sequence for word retrieval\n",
    "    size = []\n",
    "    # Gets each row\n",
    "    for row in filtered.index:\n",
    "        # Gets each title in row\n",
    "        title = filtered['name'][row]\n",
    "        # Gets the tag list\n",
    "        tag_list = filtered['Found'][row]\n",
    "        pairs = []\n",
    "        for tag_found in tag_list:\n",
    "            tag_word = tag_found.split()\n",
    "            lem = len(tag_word)\n",
    "            pairs.append(lem)\n",
    "        size.append(pairs)\n",
    "    filtered['size'] = size\n",
    "\n",
    "    # Gets the shortened phrases according to the tag length's sequence\n",
    "    new_phrases = []\n",
    "    for row in filtered.index:\n",
    "        title = filtered['name'][row]\n",
    "        # Gets the tag list\n",
    "        size = filtered['size'][row]\n",
    "        title_split = title.split()\n",
    "        for each_size in size:\n",
    "            shorter = []\n",
    "            i = 0\n",
    "            while(i < each_size):\n",
    "                shorter.append(title_split[i])\n",
    "                i = i + 1           \n",
    "            short_word = ' '.join(shorter)\n",
    "            new_phrases.append(short_word)\n",
    "    possible_title = pd.DataFrame()\n",
    "    possible_title['possible'] = new_phrases\n",
    "\n",
    "    # Removes uncommon words (e.g. weird English words that pass the first filter)\n",
    "    commonality = []\n",
    "    for title in possible_title['possible']:\n",
    "        title_split = title.split()\n",
    "        common = 1\n",
    "        for word in title_split:\n",
    "            if(word not in relevant):\n",
    "                common = 0\n",
    "        commonality.append(common)\n",
    "    possible_title['common'] = commonality\n",
    "    # The final possible titles that make sense as words AND phrases\n",
    "    possible_title = possible_title[possible_title['common'] == 1]\n",
    "    possible_title = possible_title.drop(columns=['common'])\n",
    "    final_names.append(np.array(possible_title['possible'])) \n",
    "    w = w + 1\n",
    "\n",
    "# Puts the phrases into a a DataFrame for easier visualization\n",
    "s = pd.Series(final_names) \n",
    "df = pd.DataFrame(s.values.tolist(), index=s.index)\n",
    "result = df.transpose()\n",
    "result = result.rename(columns={0: 'Temp 0.2'})\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f0cfb",
   "metadata": {},
   "source": [
    "# Refines the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e165e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parses through each generated temperature to get best generated output\n",
    "w = 0\n",
    "final_names = []\n",
    "while w < 1:\n",
    "    # =====================================================================\n",
    "    # Part I: Organizing/Finding Possible Tag Sequences\n",
    "    # =====================================================================\n",
    "    \n",
    "    # Gets the data from the right column\n",
    "    str_num = str(w)\n",
    "    column_name = 'temp: ' + str_num\n",
    "    temp = pd.DataFrame()\n",
    "    temp['name'] = gen_names[column_name]\n",
    "\n",
    "    # Parses the generated text into a list of tokens\n",
    "    lower = list(map(lambda x: x.lower().split(), temp['name']))\n",
    "    temp['Token'] = lower\n",
    "\n",
    "    # Gets the POS tagging of each word within the generated text\n",
    "    tag_sentence = list()\n",
    "    for array in lower:\n",
    "        tag = pos_tag(array)\n",
    "        tags_only = [x[1] for x in tag]\n",
    "        tag_sentence.append(tags_only) \n",
    "\n",
    "    # Turns the tag array into a tag string\n",
    "    tag_string = []\n",
    "    for listz in tag_sentence:\n",
    "        tag = \" \".join(listz)\n",
    "        tag_string.append(tag) \n",
    "    temp['Tag'] = tag_string\n",
    "\n",
    "    # Checks if a tag sequence of most popular titles matches a tag sequence of the generated title\n",
    "    greater_list = []\n",
    "    for gen_tag in temp['Tag']:\n",
    "        tags_found = []\n",
    "        for tag in top_20:\n",
    "            # Gets length of tag\n",
    "            tag_len = len(tag)\n",
    "            shortenz = gen_tag[0:tag_len]\n",
    "            # If the tag sequences match, add that tag to a list\n",
    "            if shortenz == tag:\n",
    "                tags_found.append(shortenz)\n",
    "        # Appends each found sub tag sequence to the title for tracking\n",
    "        greater_list.append(tags_found)\n",
    "    temp['Found'] = greater_list\n",
    "    # Makes a new DataFrame with the titles and their discovered tag sequence\n",
    "    newer = temp[['name','Found']].copy()\n",
    "\n",
    "    # Removes entries that had no found tag sequence\n",
    "    newer['Found'] = newer['Found'].apply(\n",
    "        lambda x: 'NaN' if len(x)==0 else x)\n",
    "    newer = newer[newer['Found'] != 'NaN']\n",
    "    \n",
    "    # =====================================================================\n",
    "    # Part II: Filtering/Refining the Generated Text\n",
    "    # =====================================================================\n",
    "    \n",
    "    # Filters out words that don't make sense/aren't in the English Dictionary\n",
    "    truth_list = []\n",
    "    for name in newer['name']:\n",
    "        # Each name starts out as True\n",
    "        nope = 1\n",
    "        name_list = name.split()\n",
    "        for word in name_list: \n",
    "            truth = d.check(word)\n",
    "            if word == 'i':\n",
    "                 nope = 0\n",
    "            # If word is not in the English Dictionary then flags\n",
    "            if truth is False:\n",
    "                nope = 0\n",
    "        truth_list.append(nope)\n",
    "    # Appends all flags to DataFrame and removes gibberish\n",
    "    newer['real'] = truth_list\n",
    "    filtered = newer[newer['real'] == 1]\n",
    "\n",
    "    # Grabs the length of the tag sequence for word retrieval\n",
    "    size = []\n",
    "    # Gets each row\n",
    "    for row in filtered.index:\n",
    "        # Gets each title in row\n",
    "        title = filtered['name'][row]\n",
    "        # Gets the tag list\n",
    "        tag_list = filtered['Found'][row]\n",
    "        pairs = []\n",
    "        for tag_found in tag_list:\n",
    "            tag_word = tag_found.split()\n",
    "            lem = len(tag_word)\n",
    "            pairs.append(lem)\n",
    "        size.append(pairs)\n",
    "    filtered['size'] = size\n",
    "\n",
    "    # Gets the shortened phrases according to the tag length's sequence\n",
    "    new_phrases = []\n",
    "    for row in filtered.index:\n",
    "        title = filtered['name'][row]\n",
    "        # Gets the tag list\n",
    "        size = filtered['size'][row]\n",
    "        title_split = title.split()\n",
    "        for each_size in size:\n",
    "            shorter = []\n",
    "            i = 0\n",
    "            while(i < each_size):\n",
    "                shorter.append(title_split[i])\n",
    "                i = i + 1           \n",
    "            short_word = ' '.join(shorter)\n",
    "            new_phrases.append(short_word)\n",
    "    possible_title = pd.DataFrame()\n",
    "    possible_title['possible'] = new_phrases\n",
    "\n",
    "    # Removes uncommon words (e.g. weird English words that pass the first filter)\n",
    "    commonality = []\n",
    "    for title in possible_title['possible']:\n",
    "        title_split = title.split()\n",
    "        common = 1\n",
    "        for word in title_split:\n",
    "            if(word not in relevant):\n",
    "                common = 0\n",
    "        commonality.append(common)\n",
    "    possible_title['common'] = commonality\n",
    "    # The final possible titles that make sense as words AND phrases\n",
    "    possible_title = possible_title[possible_title['common'] == 1]\n",
    "    possible_title = possible_title.drop(columns=['common'])\n",
    "    final_names.append(np.array(possible_title['possible'])) \n",
    "    w = w + 1\n",
    "\n",
    "# Puts the phrases into a a DataFrame for easier visualization\n",
    "s = pd.Series(final_names) \n",
    "df = pd.DataFrame(s.values.tolist(), index=s.index)\n",
    "result = df.transpose()\n",
    "result = result.rename(columns={0: 'Temp 0.2'})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fda5dda",
   "metadata": {},
   "source": [
    "# Saves the model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile_metris\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064e610a",
   "metadata": {},
   "source": [
    "# Grabs the first word of the most popular albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the first word of the most popular song albums\n",
    "first_words = pd.read_csv('song_first.csv')\n",
    "firsties = first_words['first_word']\n",
    "firsties.tolist()\n",
    "names = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76777978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gets the most popular first words and uses that first word \n",
    "# to generate a title and stores in a DataFrame() to give the\n",
    "# model a good reference to build generated content from\n",
    "i = 2\n",
    "for temp in [0.2,0.3,0.4,0.5]:\n",
    "    possible_name = []\n",
    "    for word in firsties:\n",
    "        # Gets a generated sentence\n",
    "        sentence = generate_text(model, start_string=word,t=temp)\n",
    "    \n",
    "        possible_name.append(sentence)\n",
    "    # Adds as a column\n",
    "    names['temp: '+ str(i)] = possible_name\n",
    "    # For column name\n",
    "    i = i + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e99a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d587c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs the generated title gibberish into a .csv\n",
    "names.to_csv('words2_5.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
